# HCache: A Hierarchical KV Cache Design Supporting Hierarchical Quantization

**Abstract**

Key-Value (KV) caches are widely used to optimize Large Language Model (LLM) inference engines by retaining intermediate states and reducing unnecessary recomputation. However, since KV caches scale linearly with the inference sequence length and batch size, it has become a frequent GPU memory bottleneck in practice and has inspired many recent works on KV cache compression. In particular, popular compression techniques such as KV cache quantization and KV eviction policies involve sacrificing model inference accuracy in the process, which prompted further works to focus on finding the balance between compressing the KV cache and retaining model inference accuracy. However, existing solutions lack cache-level flexibility and quantization granularity, which is an unexplored area of optimization that can yield substantial improvements in inference accuracy without increasing the KV cache size. This paper introduces HCache, a novel KV cache design supporting hierarchical quantization levels that are configurable for specific workloads, models, and other use cases. HCaches with fine-grained quantization configurations are evaluated under multiple different settings and show substantial improvements to inference accuracy and generation quality; achieving up to 4x the inference accuracy of existing KV cache structures of the same size.
